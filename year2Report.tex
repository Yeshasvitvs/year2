\documentclass[12pt,a4paper]{report}
\usepackage[T1]{fontenc}
\usepackage{array}
\usepackage{mathptmx}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{kpfonts}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{url}
\usepackage{float}
\usepackage{lscape}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\def\thesection{\arabic{section}}
\def\thesubsection{\thesection.\arabic{subsection}}
\setcounter{secnumdepth}{3}
\setlength{\parindent}{4em}
\setlength{\parskip}{1em}
\begin{document}
\begin{titlepage}
\begin{figure}
\hspace*{0.5cm}\includegraphics[scale=0.9]{logos/logos1.pdf}
\end{figure}
   \vspace*{\stretch{0.2}}
   \begin{center}
   	\Huge\textbf{Annual Report}\\
   	\vspace*{2cm}
   	\Large\textbf{Control and Representation of Articulated Objects: Insights from robots}\\
   	\vspace*{2cm}
     \Large{PhD Program in Bioengineering Robotics}\\
     \Large{Curriculum: Cognitive Robotics, Interaction and Rehabilitation Technologies} \\ 
     \vspace*{2cm}
      \large{Student: Venkata Sai Yeshasvi Tirupachuri}\\
      \large{Cycle: XXXI}\\
      \vspace{0.1cm}
      \begin{frame}
      \small
      {\centering Tutors:\par}
      \begin{tabular}[t]{@{}l@{\hspace{3pt}}p{.6\textwidth}@{}}
             {\centering \hspace{0.9cm} Dr. Daniele Pucci} \\
             {\centering \hspace{0.1cm} Prof. Gabriel Baud-Bovy} \\
             {\centering \hspace{0.6cm} Prof. Guilio Sandini} \\
      \end{tabular}%
      \end{frame}
      
      \large{Year: 2}\\
      \large{EMail : \href{mailto:yeshasvi.tirupachuri@iit.it}{yeshasvi.tirupachuri@iit.it} \\ Mobile: +39-327-974-3351}
   \end{center}
   \vspace*{\stretch{2.0}}
\end{titlepage}
\tableofcontents
\pagebreak
\section{Research Activity}
\subsection{Scientific and Technological Objectives}

To be effective in an ever changing complex environments that humans live, robots need to have certain cognitive capabilities on par with humans. Most importantly fruitful interactions with the complex dynamic environment is very crucial. 

The research network PACE(Perception and Action in Complex Environments) deals with investigations into human motor control in highly complex environments. This is a fairly complex topic which can be explored at different fronts ranging from theoretical models to rehabilitation technologies. Coming from engineering and robotics background, I strongly believe that investigations in several areas of PACE research can result in endowing robots with humanlike cognitive capabilities for interaction with dynamic and complex environments. 

The prime case of complex environments considered in my research topic is articulated objects, like doors, scissors etc. Because of their structural rigidity, most robots strain to manipulate kinematically-constrained articulated objects without causing large interaction forces. simply put these objects have different parts attached through joint(s) and hence the motion of the object parts is restricted according to the joint type(s). We humans with excellent perceptual and motor skills learn to recognize and manipulate them easily overtime. The same capability for robots is very much needed to be effective and productive in natural environments. Hence the title of my topic is ``Control and Representation of Articulated Objects: Insights from robots". \\

The technical objectives can be summarized as 
\begin{itemize}
	\item A computational framework for articulation object model estimation
	\item Implementing cognitive models of sensorimotor signal processing in interaction 
	\item Validating the framework with experiments using humanoid iCub robot
\end{itemize}
\subsection{State of the art and Innovation}
State-of-the-art research in articulation model estimation has been mainly focused on exploting visual perception. For effective manipulation of articulation objects adequate information of the shape, pose and the kinematic structure of the objects and its parts is crucial. Pose tracking generally assumes the shape of the objects or object parts and utilizes segmentation in the visual input. Shape reconstruction can be achieved using integration of multiple poses and then the kinematic structure of an articulated object can be achieved using the above information on the object parts.

The segmentation problem deals with extracting the region of interest in the visual input which contains the object to track. 
\begin{itemize}
	\item Single image based segmentation ~\cite{mishra2009active},~\cite{papon2013voxel}
	\item Motion based segmentation ~\cite{chien2002efficient}
\end{itemize}

Pose estimation or visual tracking can be achieved using
\begin{itemize}
	\item Based on a known shape model of the object ~\cite{wuthrich2013probabilistic}
	\item Based on the sufficient texture extracting features ~\cite{martin2014online} or optical flow ~\cite{stuckler2013efficient},~\cite{ochs2014segmentation}
\end{itemize} 

Shape reconstruction outputs a 3D model of an object by stitching together several object views into one coherent shape model using relative poses w.r.t the visual system. Object views are obtained using information from segmentation and pose estimation~\cite{krainin2011manipulator}. Visual SLAM is a popular approach combining tracking and shape reconstruction but for an entire scene assumed to be static. An integrated approach for the visual perception of articulated objects is recently proposed in ~\cite{martinintegrated} which combines tracking, shape reconstruction and estimation of kinematic structures. 

Doors are the most likely experienced articulated objects in many robotic applications like rescue scenarios, elderly care, hospitality etc., and the earliest investigations started with a simple strategy to open an unknown door ~\cite{niemeyer1997simple}. More recently, Advait Jain and Charles C. Kemp proposed the concept of equilibrium point control (EPC) ~\cite{jain2009pulling} for the specific task of opening novel doors and drawers. In addition they implemented an articulation model estimation algorithm using the end-effector trajectory, assuming a stable grasp and planar motion. The algorithm returns an estimate of the location of the axis of rotation and the radius. Prismatic joint is estimated as a rotational joint with large radius. 

Dov Katz and Oliver Brock ~\cite{katz2007interactive} introduced the idea of interactive perception paradigm and highlights the need for extracting task specific perceptual information using the manipulation capabilities of a robot by interacting with the environment. Some of the limitations in \cite{katz2007interactive} are removed in ~\cite{katz2008manipulating} and a robust algorithm is presented. Feature trajectories of moving objects are extracted using optical flow based tracking and a graph is built where vertices represent the tracked features. Highly connected sub-graphs are separated using min-cut algorithm and these sub-graphs represent the object parts. Then the articulation models are extracted from the information contained in the graph. Rotational joint is identified by rotating centers between two sub-graphs and prismatic joint by shifting movements of sub-graphs. They successfully demonstrated the use of interactive perception in extracting kinematic model of various tools to build a Denavit-Hartenberg (DH) parameter model and then use it to operate a tool. Furthermore, a symbolic learning based approach to manipulation is presented in ~\cite{brock2009learning} which uses relational representations of kinematic structures that are grounded using perceptual and interaction capabilities of the robot. They successfully demonstrated learning and generalization of manipulation knowledge to previously unseen objects. 

Strum et al. ~\cite{sturm2009learning} proposed a probabilistic learning framework using noisy 3D pose observations of object parts. They implemented predefined candidate joint models with parameters and also a non-parametric gaussian process model to which observed 3D pose trajectory data of object parts is fit to find kinematic structures of kinematic trees. Unlike the previous works this is a passive approach and they use pose observations from motion capture system, when the articulated object is manipulated by someone, and simulated data to validate their framework. Further, a stereo camera system is used to get dense depth images as input ~\cite{sturm20103d}. An approach to direct interaction with articulated objects to learn articulation models and improve based on experience is presented in ~\cite{sturm2010operating}. Furthermore, a unified framework with several extensions like dealing with kinematic loops and extended set of experiments is presented in ~\cite{sturm2011probabilistic}.

Stefan et al. ~\cite{otte2014entropy} introduced the exploration challenge for robots where the task is to perform explorative actions and learn the structure of the environment. One of their main contributions is probabilistic belief representation of articulation models including properties like friction and joint limits. They successfully demonstrated how the behaviour emerged from entropy-based exploration is more informative than explorative strategies based on heuristics. Hausman et at. ~\cite{hausman2015active} proposed a partcile filter based approach where the articulations models are represented as particles and implemented a probabilistic framework based on the concept of interactive perception to integrate visual observations with manipulation feedback from the robot. They also presented best action selection methods based on entropy and information gain which guides the robot to perform the most useful interactions with the object to reduce its uncertainty.

Martin-Martin Roberto and Oliver Brock ~\cite{martin2014online} presented an online multi-level recursive estimation algorithm considering task-specific priors based on the concept of interactive perception. They used series of RGB-D image data as input to estimate articulation models including the joint configuration. Furthermore, they extended their approach ~\cite{martin2017building} integrating information from vision, force-torque sensing and proprioception. In addition to kinematic model estimation they also generated a dynamic model of the articulated object. 

The choice of a well established framework on human motor control to facilitate the basic software infrastructure for my research is a crucial step. Towards this end I did review of papers on the framework of Active Inference ~\cite{friston2015active},~\cite{friston2011action} put forward by Prof. Karl Friston. Also I studied the framework of Passive Motion Paradigm(PMP) ~\cite{mohan2011passive}, ~\cite{mohan2013inference} championed by our colleagues Dr. Vishwanathan Mohan and Prof. Pietro Morasso from the Italian Institute of Technology.

The momentum based topology estimation algorithm presented in this paper fundamentally differs from prior work. Visual information has been used as the most dominant sensory modality in articulation model estimation. But while manipulating an articulated object, another key information available is the interaction forces and torques. The main contribution of this paper is an articulation model estimation algorithm that uses momentum information and the interaction wrench information. To the best of our knowledge no prior work has been published which takes into account, the momentum and the interaction wrenches present while manipulating articulated objects. 
	
The novelty of my research work involves developing an articulation object model estimation framework for a highly complex humanoid robot(iCub). As a part of the Dynamic Touch and Interaction team and in collaboration with Dynamic Interaction Control  team, I am  exploring the possibilities of using interaction force along with visual perception  to estimate the kinematic structure and dynamic properties of the articulated objects. From the visual perception kinematic properties of the artiulated can be estimated but by also using interaction force we believe to estimate the dynamic properties. This facilititates for better control during object manipulation. A bayesian framework is proposed in ~\cite{nori2015simultaneous} for simultaneous state and dynamics estimation in presence of multiple redundant sensory information. But in the case of manipulation of articulated objects state information will estimated using visual inputs. 


\subsection{Methodology and Workplan}

We modelled two simple artitulated objects using Simulation Description Format (SDF) to be used in Gazebo simulation environment. The objects contain two links connected through a \textit{prismatic} joint, \textit{\textbf{P}} in one (Fig. \ref{pmodel}) and a \textit{revolute} joint, \textit{\textbf{R}} in the other  (Fig. \ref{rmodel}). The reasoning behind this modelling is that most real life articulated objects can be represented in this simplified form. The cylindrical elements in black color are the handle links and the rectangular elements in yellow color are articulated object links.

\begin{figure}[H]
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
		\includegraphics[scale=0.15]{figures/1p_2link.png}
		\caption{Prismatic Model}
		\label{pmodel}
	\end{subfigure}
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
		\includegraphics[scale=0.16]{figures/1r_2link.png}
		\caption{Revolute Model}
		\label{rmodel}
	\end{subfigure}
	\caption{Articulated Object Models}
	\label{icub_mani}
\end{figure}

The experimental scenario we want to test is as indicated in the figure \ref{icub_object}, where the icub robot is holding the articulated object at the handles and performs explorative actions to estimate the
articulation model present between the links of an articulated object.

\begin{figure}[H]
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
		\includegraphics[scale=0.22]{figures/icub_object_side.jpg}
		\caption{Side View}
		\label{pmodel}
	\end{subfigure}
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
		\includegraphics[scale=0.22]{figures/icub_object_top.jpg}
		\caption{Top View}
		\label{rmodel}
	\end{subfigure}
	\caption{Experimental Scenario: iCub manipulating an articulated object}
	\label{icub_object}
\end{figure}


I started working on the submodules of vision and force/torque in the perception module of the software architecture shown in figure \ref{SoftArc}. Obtaining robust 3D pose values of rigid bodies is still an open challenge in the field of computer vision. Vast amount of research has been carried on tracking rigid bodies either using markers, features or depth information. As visual perception is not the main goal of this work, I made use of aruco library ~\cite{Aruco2014} which is an opensource software for augumented reality applications based on OpenCV. Figure \ref{markersboard} shows an array of markers and figure \ref{marker_stapler} shows the markers attached on the two moving parts of a stapler with their detected 3D pose shows with an axes.

\begin{figure}[H]
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
		\includegraphics[scale=0.3]{figures/markersboard.png}
		\caption{Markers Board}
		\label{markersboard}
	\end{subfigure}
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
		\includegraphics[scale=0.3]{figures/stapler.png}
		\caption{Markers on a stapler}
		\label{marker_stapler}
	\end{subfigure}
	\caption{Augumented Reality Markers}
	\label{armarkers}
\end{figure}

Coming to force/torque perception, iCub robot is equipped with six proximal six-axis F/T sensors as shown in the figure. While using the iCub robot external wrenches can be estimated using the techniques develoed for whole-body control ~\cite{nori2015icub}. At this point the current icub model described using an URDF (Universal Robot Description Format) file in simulation environment did not include the hands of the robot. I created a new URDF fiels for the hands of the robot included along with collision elements and did contact tuning in gazebo simulation environment.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/icub_ft.png}
  \caption{iCub robot F/T Sensors}
  \label{icub_ft}
\end{figure}



The handle links are connected to object links through a fixed joint, where a simulated 6 axis Force-Torque sensor plugin ~\cite{hoffman2014yarp} is placed, to measure the external wrenches acting on the object links.  One of the handles is anchored to the world in gazebo through a fixed joint and this also anchors the object link attached to that handle. The reasoning behind this choice is that many articulated objects have one of their links either grounded or are quite heavy to move compared to the freely movable link. 




\subsection{Achieved results, dissemination, deviation from the original plan}
The gantt chart in the figure \ref{gantt} shows the proposed work plan during the project proposal and the actual work progress during the first year of PhD. The software architecture of the framework is not attained yet as I have been looking at multiple fronts to get a coherent understanding of the currently available frameworks and I expect to achieve this by the end of year 2016. The figure \ref{SoftArc} presents a rough software architecture indicating various modules and the submodules of the envisioned framework.

\begin{figure}[!ht]
	\centering
	\includegraphics[scale=0.3]{figures/SoftwareArch_year1.png}
	\caption{Software Architechture}		
	\label{SoftArc}
\end{figure}

Regarding implementation, I developed a module called simplePMP ~\cite{Tirupachuri:2016:Online} using iKin library module of yarp and a corresponding demo in gazebo simulation with iCub can be found at ~\cite{simplePMP_Demo:2016:Online}. Figure \ref{icub_mani1} shows the gazebo simulation environment of icub and an articulated object which is a planar 2R system(fixed). The transition from figure \ref{icub_mani1} to \ref{icub_mani2} shows iCub reaching and grasping the handle of articulated object. In this simulation the reaching movement was hard-coded as it is not the main focus of my work. The transition from figure \ref{icub_mani2} to \ref{icub_mani3} shows a movement controlled by simplePMP module. The task was to move the end effector towards iCub along a straight line. This simulation shows the ability of simplePMP to perform the motor control part of the robot while grasping an articulated object. Due to the kinematic constraints of the articulated object large interaction forces might arise at the point of contact which prevents iCub to manipulate the object as desired. This problem can be alleviated by the PMP framework which takes advantage of the robot compliance to control the iCub motion.  

In the above implementation the articulated object is considered to be a fixed base system. Technically a fixed base system is the one in which the pose of the base frame with respect to the inertial reference frame does not change. Practically the base of fixed base systems is attached to the ground tightly and is assumed that it will never move. In this case the ground acts as an infinite force generactor providing reaction forces to the system and greatly limits the passive dynamics. Regarding articulated objects the assumption of fixed base is not true in many everyday occurances and hence I am interested in exploring the idea of considering articulated objects as free floating base systems. So at present I am modelling the dynamics and running simulations on free floating articulated systems. Once this is done preliminary experiments using motor control framework of PMP and articulated objects can be performed in gazebo simulation. 

\begin{figure}[!ht]
		\centering
		\begin{subfigure}[b]{0.5\textwidth}
			\includegraphics[scale=0.299]{figures/icub1.jpg}
			\caption{  }
			\label{icub_mani1}
		\end{subfigure}
		\begin{subfigure}[b]{0.5\textwidth}
			\includegraphics[scale=0.3]{figures/icub2.jpg}
	    		\caption{  }
	    		\label{icub_mani2}
		\end{subfigure}
	    \begin{subfigure}[b]{0.5\textwidth}	
			\includegraphics[scale=0.295]{figures/icub3.jpg}
			\caption{  }
			\label{icub_mani3}
	 	\end{subfigure}		
		\caption{iCub manipulating an articulated object}
		\label{icub_mani}
\end{figure}

\begin{landscape}
	\begin{figure}[!ht]
		\centering
		\caption{Gantt Chart PhD Year II}
		\includegraphics[scale=1]{PhD_year2_gantt.pdf}
		\label{gantt}
	\end{figure}
\end{landscape}

\renewcommand{\bibsection}{\subsection{\bibname}}
\renewcommand{\bibname}{\large{References}}
\bibliographystyle{plain}
\bibliography{ref}

\section{Traning Related to the PhD Programme}
\begin{enumerate}
    \item Artificial Cognitive Systems
    \begin{itemize}
		\item Type: PhD Course
		\item Organizer: IIT
		\item Duration: 10 Hours
		\item Credits: 3
		\item Verification: Pass
	\end{itemize}
	\item An Introduction to Spatial (6D) Vectors and Their Use in Robot Dynamics
	\begin{itemize}
		\item Type: PhD Course
		\item Organizer: IIT
		\item Duration: 15 Hours
		\item Credits: 5
		\item Verification: Pass
	\end{itemize}
	\item Machine Learning: A Computational Intelligence Approach
	\begin{itemize}
		\item Type: PhD Course
		\item Organizer: UNIGE
		\item Duration: 18 Hours
		\item Credits: 6
		\item Verification: EXAM TO BE HELD ON 18-SEP-2017
	\end{itemize}
\end{enumerate}
\section{Other Activities}
\begin{itemize}
    \item PACE second Thematic Workshop(TW2) on Active Inference at l’Université de Lyon, 12 - 14 October 2016, Lyon, France
    \item Online E-Learning module on Kinematic data (arm + eye): Concepts and analysis by Prof. Joan Lopez Moliner from Universitat de Barcelona (UB), 24 November 2016
    \item Workshop on High Tech Entrepreneurship organized by IIT and University of Genoa, 28 October - 15 December 2016, Genoa, Italy
    \item PACE Second Network Meeting(NM2) at  Universitat de Barcelona (UB), 20 - 22 February 2017, Barcelona, Spain
    \item Science communication project ~\cite{SCP} of PACE in partnership with the International School for Advanced Studies - SISSA, Trieste, Italy
    \item Online E-Learning module on Introduction to robotics and haptic device control by Prof. Gabriel Baud-Bovy from IIT, Italy, 4 May 2017
    \item Online E-Learning module on Haptic devices by Prof. Gabriel Baud-Bovy from IIT, Italy, 24 May 2017
    \item Online E-Learning module on Human-robot social interactions by Dr. Alessandra Sciutti from IIT, Italy, 15 June 2017
    \item Online E-Learning module on Robot assisted rehabilitation by Dr. Jacopo Zenzeri from IIT, Italy, 20 June 2017
    \item Online E-Learning module on Control theory dy Dr. Daniele Pucci from IIT, 29 June 2017
    \item Online E-Learning module on An example of and advanced robot: iCub by Prof. Giulio Sandini and Dr. Francesco Rea from IIT, Italy, 25 July 2017
\end{itemize}
\vspace*{10cm}
Signatures, \\
\vspace*{4cm}
PhD Student \hspace*{12cm} Tutor(s)

\end{document}
